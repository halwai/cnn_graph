{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from lib import models, graph, coarsening, utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.sparse\n",
    "from scipy.io import *\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Graphs.\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n",
    "# TODO: change cgcnn for combinatorial Laplacians.\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')\n",
    "\n",
    "# Directories.\n",
    "flags.DEFINE_string('dir_data', os.path.join('..', 'data', 'mnist'), 'Directory to store data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      " No Preprocessing Required\n"
     ]
    }
   ],
   "source": [
    "# Number of Nearest Neighbors\n",
    "k = 110\n",
    "\n",
    "base_storage_dir = '/data4/abhijeet/Datasets/PASCAL_VOC/GCN/preprocessing/experiment3/'\n",
    "temp_filename = base_storage_dir + 'scikit_fc7_k_' + str(k) + '.mat'\n",
    "\n",
    "if os.path.exists(temp_filename):\n",
    "    print('File already exists\\n No Preprocessing Required')\n",
    "else:\n",
    "    print('Do preprocessing --> TODO --> call the fucntion with a specific parameter')\n",
    "    \n",
    "data = scipy.io.loadmat( temp_filename)\n",
    "\n",
    "X_train          = data['X_train']\n",
    "X_test           = data['X_test']\n",
    "train_labels     = data['Y_train']\n",
    "test_labels      = data['Y_test']\n",
    "Adjacency_test   = data['Adjacency_test']\n",
    "Adjacency_train  = data['Adjacency_train']\n",
    "\n",
    "#delete unused variables\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#define laplacians, coarsen adjacency, permutate data points.\n",
    "train_laplacians = [[] for i in range(1 + FLAGS.coarsening_levels)]\n",
    "test_laplacians = [[] for i in range(1 + FLAGS.coarsening_levels)]\n",
    "\n",
    "train_perm ,test_perm = [] ,[]\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    graphs, temp_perm = coarsening.coarsen(Adjacency_train[0][i], levels=FLAGS.coarsening_levels, self_connections=True)\n",
    "    train_perm.append(temp_perm)\n",
    "    temp_L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    temp_L = [graph.rescale_L(A) for A in temp_L]\n",
    "    #graph.plot_spectrum(L)\n",
    "    temp_L = [A.toarray() for A in temp_L]\n",
    "    for j in range(1 + FLAGS.coarsening_levels):\n",
    "        train_laplacians[j].append(temp_L[j])     \n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    graphs, temp_perm = coarsening.coarsen(Adjacency_test[0][i], levels=FLAGS.coarsening_levels, self_connections=True)\n",
    "    test_perm.append(temp_perm)\n",
    "    temp_L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    temp_L = [graph.rescale_L(A) for A in temp_L]\n",
    "    #graph.plot_spectrum(L)\n",
    "    temp_L = [A.toarray() for A in temp_L]\n",
    "    for j in range(1 + FLAGS.coarsening_levels):\n",
    "        test_laplacians[j].append(temp_L[j])     \n",
    "\n",
    "#del Adjacency_test, Adjacency_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5\n",
      "4952\n",
      "(112, 112)\n",
      "4952\n",
      "(56, 56)\n",
      "4952\n",
      "(28, 28)\n",
      "4952\n",
      "(14, 14)\n"
     ]
    }
   ],
   "source": [
    "print(type(test_laplacians))\n",
    "print(len(test_laplacians))\n",
    "for i in range(4):\n",
    "    print(len(test_laplacians[i]))\n",
    "    print(test_laplacians[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 <class 'list'>\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_perm[0]),type(train_perm))\n",
    "train_data = np.zeros((X_train.shape[0], len(train_perm[0]), X_train.shape[2]))\n",
    "test_data = np.zeros((X_test.shape[0], len(test_perm[0]), X_test.shape[2]))\n",
    "for i in range(train_data.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    train_data[i,:,:] = coarsening.perm_data_point(X_train[i,:,:], train_perm[i])\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    test_data[i,:,:] = coarsening.perm_data_point(X_test[i,:,:], test_perm[i])\n",
    "\n",
    "val_data = test_data.copy()\n",
    "val_laplacians = test_laplacians.copy()\n",
    "val_labels = test_labels.copy()\n",
    "\n",
    "#del X_train, Y_train, X_test, Y_test \n",
    "#del perm_train, perm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[ array([[-0.00916375, -0.0091022 , -0.00909718, ..., -0.00908158,\n",
      "        -0.00908857, -0.00909073],\n",
      "       [-0.0091022 , -0.00916535, -0.00909755, ..., -0.00908056,\n",
      "        -0.0090868 , -0.00907949],\n",
      "       [-0.00909718, -0.00909755, -0.00916237, ..., -0.00908507,\n",
      "        -0.00908112, -0.00908566],\n",
      "       ..., \n",
      "       [-0.00908158, -0.00908056, -0.00908507, ..., -0.00916809,\n",
      "        -0.00909857, -0.00908995],\n",
      "       [-0.00908857, -0.0090868 , -0.00908112, ..., -0.00909857,\n",
      "        -0.0091637 , -0.00909815],\n",
      "       [-0.00909073, -0.00907949, -0.00908566, ..., -0.00908995,\n",
      "        -0.00909815, -0.00916497]])\n",
      " array([[-0.01826675, -0.01819766, -0.0181865 , ..., -0.01817435,\n",
      "        -0.01817092, -0.0181728 ],\n",
      "       [-0.01819766, -0.01826438, -0.01819268, ..., -0.01817492,\n",
      "        -0.0181736 , -0.01816924],\n",
      "       [-0.0181865 , -0.01819268, -0.01825985, ..., -0.01818277,\n",
      "        -0.01818589, -0.01818156],\n",
      "       ..., \n",
      "       [-0.01817435, -0.01817492, -0.01818277, ..., -0.0182632 ,\n",
      "        -0.01818596, -0.01817506],\n",
      "       [-0.01817092, -0.0181736 , -0.01818589, ..., -0.01818596,\n",
      "        -0.01826262, -0.01819283],\n",
      "       [-0.0181728 , -0.01816924, -0.01818156, ..., -0.01817506,\n",
      "        -0.01819283, -0.01826249]])\n",
      " array([[-0.03646322, -0.02572397, -0.03637357, -0.03636643, -0.03633123,\n",
      "        -0.03633816, -0.0363571 , -0.03637708, -0.03634832, -0.03636555,\n",
      "        -0.03636004, -0.03637941, -0.0363624 , -0.03636731, -0.03634867,\n",
      "        -0.03635756, -0.03635543, -0.03636997, -0.03636172, -0.03637976,\n",
      "        -0.03635318, -0.03636376, -0.03635525, -0.03635532, -0.03635743,\n",
      "        -0.03636291, -0.03635411, -0.03634328],\n",
      "       [-0.02572397, -0.01825985, -0.02571733, -0.02572009, -0.0257011 ,\n",
      "        -0.02571345, -0.02572037, -0.025718  , -0.02570721, -0.02570755,\n",
      "        -0.02571229, -0.02571573, -0.02571101, -0.02572136, -0.02570773,\n",
      "        -0.02570632, -0.02571512, -0.02571754, -0.02570727, -0.02571709,\n",
      "        -0.02571142, -0.02570929, -0.02571134, -0.02571792, -0.02572083,\n",
      "        -0.02571189, -0.02571388, -0.02571567],\n",
      "       [-0.03637357, -0.02571733, -0.03645539, -0.03638528, -0.03634651,\n",
      "        -0.03635143, -0.03636002, -0.03637104, -0.0363654 , -0.03636938,\n",
      "        -0.03637504, -0.03636473, -0.03635717, -0.03636464, -0.03636096,\n",
      "        -0.03635573, -0.03636388, -0.03638073, -0.03636717, -0.03637122,\n",
      "        -0.03635371, -0.03635923, -0.0363634 , -0.03636282, -0.03636745,\n",
      "        -0.03637262, -0.0363626 , -0.03635426],\n",
      "       [-0.03636643, -0.02572009, -0.03638528, -0.03646233, -0.03634372,\n",
      "        -0.03634541, -0.03636765, -0.03636637, -0.03636707, -0.03636471,\n",
      "        -0.03636971, -0.03635809, -0.0363539 , -0.03635896, -0.03635506,\n",
      "        -0.03635436, -0.03636504, -0.03637283, -0.0363638 , -0.03635736,\n",
      "        -0.03636342, -0.0363591 , -0.03635433, -0.03636127, -0.03636679,\n",
      "        -0.03636844, -0.03636914, -0.03635047],\n",
      "       [-0.03633123, -0.0257011 , -0.03634651, -0.03634372, -0.03646353,\n",
      "        -0.03636797, -0.03636017, -0.03635641, -0.03635023, -0.0363366 ,\n",
      "        -0.03634614, -0.03634671, -0.03633832, -0.03634968, -0.03634711,\n",
      "        -0.0363564 , -0.03634906, -0.03635304, -0.03633598, -0.03633624,\n",
      "        -0.03635409, -0.03635911, -0.03634552, -0.03634514, -0.03633983,\n",
      "        -0.03634657, -0.03635533, -0.03636327],\n",
      "       [-0.03633816, -0.02571345, -0.03635143, -0.03634541, -0.03636797,\n",
      "        -0.03646825, -0.03637243, -0.03636109, -0.03634944, -0.03634045,\n",
      "        -0.03634647, -0.03634704, -0.03635418, -0.03637643, -0.03635525,\n",
      "        -0.03634617, -0.03633998, -0.03635125, -0.03633734, -0.03633172,\n",
      "        -0.03635148, -0.03635228, -0.03636383, -0.03636348, -0.03635918,\n",
      "        -0.03636098, -0.0363543 , -0.0363674 ],\n",
      "       [-0.0363571 , -0.02572037, -0.03636002, -0.03636765, -0.03636017,\n",
      "        -0.03637243, -0.03645898, -0.03637424, -0.03636602, -0.036357  ,\n",
      "        -0.03636708, -0.03636436, -0.03636095, -0.03637874, -0.0363528 ,\n",
      "        -0.03636933, -0.03634444, -0.03635816, -0.0363482 , -0.03633718,\n",
      "        -0.03636287, -0.03636008, -0.03635497, -0.03637478, -0.03634796,\n",
      "        -0.03637184, -0.03634262, -0.03636692],\n",
      "       [-0.03637708, -0.025718  , -0.03637104, -0.03636637, -0.03635641,\n",
      "        -0.03636109, -0.03637424, -0.03645089, -0.0363551 , -0.03635753,\n",
      "        -0.0363771 , -0.03636681, -0.03635709, -0.0363747 , -0.03635008,\n",
      "        -0.03635041, -0.03635791, -0.03637383, -0.0363612 , -0.0363684 ,\n",
      "        -0.03636064, -0.03636481, -0.03636022, -0.0363677 , -0.03635731,\n",
      "        -0.03637001, -0.03636   , -0.03636424],\n",
      "       [-0.03634832, -0.02570721, -0.0363654 , -0.03636707, -0.03635023,\n",
      "        -0.03634944, -0.03636602, -0.0363551 , -0.03646537, -0.03638997,\n",
      "        -0.036372  , -0.03636579, -0.03634737, -0.03634818, -0.03637442,\n",
      "        -0.03636149, -0.03633902, -0.03634561, -0.03637132, -0.03634687,\n",
      "        -0.03634824, -0.03635311, -0.03635   , -0.03634708, -0.03635443,\n",
      "        -0.03635239, -0.03635894, -0.03635385],\n",
      "       [-0.03636555, -0.02570755, -0.03636938, -0.03636471, -0.0363366 ,\n",
      "        -0.03634045, -0.036357  , -0.03635753, -0.03638997, -0.03645696,\n",
      "        -0.03637403, -0.03637253, -0.03635761, -0.03635741, -0.03636378,\n",
      "        -0.0363644 , -0.03634725, -0.03634732, -0.03637181, -0.03635635,\n",
      "        -0.03634581, -0.0363561 , -0.03634229, -0.0363388 , -0.03635919,\n",
      "        -0.03636442, -0.03635113, -0.03634824],\n",
      "       [-0.03636004, -0.02571229, -0.03637504, -0.03636971, -0.03634614,\n",
      "        -0.03634647, -0.03636708, -0.0363771 , -0.036372  , -0.03637403,\n",
      "        -0.03645761, -0.03637813, -0.03635438, -0.0363643 , -0.03636256,\n",
      "        -0.03636145, -0.03636442, -0.03637043, -0.03636213, -0.03637439,\n",
      "        -0.03635114, -0.03635933, -0.03636049, -0.03635677, -0.03634929,\n",
      "        -0.03637254, -0.03636256, -0.03636343],\n",
      "       [-0.03637941, -0.02571573, -0.03636473, -0.03635809, -0.03634671,\n",
      "        -0.03634704, -0.03636436, -0.03636681, -0.03636579, -0.03637253,\n",
      "        -0.03637813, -0.03645259, -0.03637125, -0.03636698, -0.03636484,\n",
      "        -0.03637661, -0.0363539 , -0.03636126, -0.03636559, -0.03637048,\n",
      "        -0.03634891, -0.03636372, -0.03635363, -0.03634416, -0.03635403,\n",
      "        -0.03636408, -0.0363625 , -0.03635569],\n",
      "       [-0.0363624 , -0.02571101, -0.03635717, -0.0363539 , -0.03633832,\n",
      "        -0.03635418, -0.03636095, -0.03635709, -0.03634737, -0.03635761,\n",
      "        -0.03635438, -0.03637125, -0.03645308, -0.03637451, -0.03636205,\n",
      "        -0.03636947, -0.03634218, -0.03634945, -0.03634891, -0.03634739,\n",
      "        -0.03634605, -0.03635274, -0.03634849, -0.03634163, -0.03635405,\n",
      "        -0.0363641 , -0.0363532 , -0.03635214],\n",
      "       [-0.03636731, -0.02572136, -0.03636464, -0.03635896, -0.03634968,\n",
      "        -0.03637643, -0.03637874, -0.0363747 , -0.03634818, -0.03635741,\n",
      "        -0.0363643 , -0.03636698, -0.03637451, -0.03644786, -0.03636189,\n",
      "        -0.03636464, -0.03635185, -0.03636572, -0.0363501 , -0.03635387,\n",
      "        -0.03635733, -0.03636076, -0.03636171, -0.03636602, -0.03635805,\n",
      "        -0.0363809 , -0.03635918, -0.03636181],\n",
      "       [-0.03634867, -0.02570773, -0.03636096, -0.03635506, -0.03634711,\n",
      "        -0.03635525, -0.0363528 , -0.03635008, -0.03637442, -0.03636378,\n",
      "        -0.03636256, -0.03636484, -0.03636205, -0.03636189, -0.03645501,\n",
      "        -0.0363697 , -0.03636064, -0.03635121, -0.03636243, -0.03635904,\n",
      "        -0.03634409, -0.03635511, -0.03636507, -0.03634743, -0.0363548 ,\n",
      "        -0.03635506, -0.03636524, -0.03635452],\n",
      "       [-0.03635756, -0.02570632, -0.03635573, -0.03635436, -0.0363564 ,\n",
      "        -0.03634617, -0.03636933, -0.03635041, -0.03636149, -0.0363644 ,\n",
      "        -0.03636145, -0.03637661, -0.03636947, -0.03636464, -0.0363697 ,\n",
      "        -0.03646485, -0.03634981, -0.03634969, -0.03636563, -0.03635673,\n",
      "        -0.0363696 , -0.03636993, -0.03635689, -0.03634908, -0.03634735,\n",
      "        -0.03636742, -0.03636003, -0.03636863],\n",
      "       [-0.03635543, -0.02571512, -0.03636388, -0.03636504, -0.03634906,\n",
      "        -0.03633998, -0.03634444, -0.03635791, -0.03633902, -0.03634725,\n",
      "        -0.03636442, -0.0363539 , -0.03634218, -0.03635185, -0.03636064,\n",
      "        -0.03634981, -0.03645873, -0.03638315, -0.03635361, -0.03637026,\n",
      "        -0.03634628, -0.03634905, -0.03636012, -0.03634643, -0.03635233,\n",
      "        -0.03634939, -0.03636297, -0.03634642],\n",
      "       [-0.03636997, -0.02571754, -0.03638073, -0.03637283, -0.03635304,\n",
      "        -0.03635125, -0.03635816, -0.03637383, -0.03634561, -0.03634732,\n",
      "        -0.03637043, -0.03636126, -0.03634945, -0.03636572, -0.03635121,\n",
      "        -0.03634969, -0.03638315, -0.03645583, -0.03636611, -0.03637794,\n",
      "        -0.03636502, -0.03636131, -0.03636895, -0.03636764, -0.03636817,\n",
      "        -0.03637125, -0.03637216, -0.0363576 ],\n",
      "       [-0.03636172, -0.02570727, -0.03636717, -0.0363638 , -0.03633598,\n",
      "        -0.03633734, -0.0363482 , -0.0363612 , -0.03637132, -0.03637181,\n",
      "        -0.03636213, -0.03636559, -0.03634891, -0.0363501 , -0.03636243,\n",
      "        -0.03636563, -0.03635361, -0.03636611, -0.03645334, -0.03637405,\n",
      "        -0.03637238, -0.03636998, -0.03636324, -0.03636335, -0.03636714,\n",
      "        -0.03636547, -0.03636156, -0.03635689],\n",
      "       [-0.03637976, -0.02571709, -0.03637122, -0.03635736, -0.03633624,\n",
      "        -0.03633172, -0.03633718, -0.0363684 , -0.03634687, -0.03635635,\n",
      "        -0.03637439, -0.03637048, -0.03634739, -0.03635387, -0.03635904,\n",
      "        -0.03635673, -0.03637026, -0.03637794, -0.03637405, -0.03646357,\n",
      "        -0.03635458, -0.03636549, -0.03637265, -0.03636416, -0.03635572,\n",
      "        -0.036359  , -0.03637701, -0.03635811],\n",
      "       [-0.03635318, -0.02571142, -0.03635371, -0.03636342, -0.03635409,\n",
      "        -0.03635148, -0.03636287, -0.03636064, -0.03634824, -0.03634581,\n",
      "        -0.03635114, -0.03634891, -0.03634605, -0.03635733, -0.03634409,\n",
      "        -0.0363696 , -0.03634628, -0.03636502, -0.03637238, -0.03635458,\n",
      "        -0.03646171, -0.03638563, -0.0363722 , -0.03637931, -0.03635554,\n",
      "        -0.03637428, -0.03636849, -0.0363825 ],\n",
      "       [-0.03636376, -0.02570929, -0.03635923, -0.0363591 , -0.03635911,\n",
      "        -0.03635228, -0.03636008, -0.03636481, -0.03635311, -0.0363561 ,\n",
      "        -0.03635933, -0.03636372, -0.03635274, -0.03636076, -0.03635511,\n",
      "        -0.03636993, -0.03634905, -0.03636131, -0.03636998, -0.03636549,\n",
      "        -0.03638563, -0.03645411, -0.0363649 , -0.03637134, -0.03635566,\n",
      "        -0.03637415, -0.03637462, -0.03637368],\n",
      "       [-0.03635525, -0.02571134, -0.0363634 , -0.03635433, -0.03634552,\n",
      "        -0.03636383, -0.03635497, -0.03636022, -0.03635   , -0.03634229,\n",
      "        -0.03636049, -0.03635363, -0.03634849, -0.03636171, -0.03636507,\n",
      "        -0.03635689, -0.03636012, -0.03636895, -0.03636324, -0.03637265,\n",
      "        -0.0363722 , -0.0363649 , -0.0364493 , -0.03638367, -0.03635152,\n",
      "        -0.03636601, -0.03636946, -0.03637143],\n",
      "       [-0.03635532, -0.02571792, -0.03636282, -0.03636127, -0.03634514,\n",
      "        -0.03636348, -0.03637478, -0.0363677 , -0.03634708, -0.0363388 ,\n",
      "        -0.03635677, -0.03634416, -0.03634163, -0.03636602, -0.03634743,\n",
      "        -0.03634908, -0.03634643, -0.03636764, -0.03636335, -0.03636416,\n",
      "        -0.03637931, -0.03637134, -0.03638367, -0.03646466, -0.0363578 ,\n",
      "        -0.03637053, -0.03636089, -0.0363796 ],\n",
      "       [-0.03635743, -0.02572083, -0.03636745, -0.03636679, -0.03633983,\n",
      "        -0.03635918, -0.03634796, -0.03635731, -0.03635443, -0.03635919,\n",
      "        -0.03634929, -0.03635403, -0.03635405, -0.03635805, -0.0363548 ,\n",
      "        -0.03634735, -0.03635233, -0.03636817, -0.03636714, -0.03635572,\n",
      "        -0.03635554, -0.03635566, -0.03635152, -0.0363578 , -0.03645081,\n",
      "        -0.0363692 , -0.03636612, -0.03635619],\n",
      "       [-0.03636291, -0.02571189, -0.03637262, -0.03636844, -0.03634657,\n",
      "        -0.03636098, -0.03637184, -0.03637001, -0.03635239, -0.03636442,\n",
      "        -0.03637254, -0.03636408, -0.0363641 , -0.0363809 , -0.03635506,\n",
      "        -0.03636742, -0.03634939, -0.03637125, -0.03636547, -0.036359  ,\n",
      "        -0.03637428, -0.03637415, -0.03636601, -0.03637053, -0.0363692 ,\n",
      "        -0.03644986, -0.03636281, -0.03636658],\n",
      "       [-0.03635411, -0.02571388, -0.0363626 , -0.03636914, -0.03635533,\n",
      "        -0.0363543 , -0.03634262, -0.03636   , -0.03635894, -0.03635113,\n",
      "        -0.03636256, -0.0363625 , -0.0363532 , -0.03635918, -0.03636524,\n",
      "        -0.03636003, -0.03636297, -0.03637216, -0.03636156, -0.03637701,\n",
      "        -0.03636849, -0.03637462, -0.03636946, -0.03636089, -0.03636612,\n",
      "        -0.03636281, -0.03645844, -0.03636869],\n",
      "       [-0.03634328, -0.02571567, -0.03635426, -0.03635047, -0.03636327,\n",
      "        -0.0363674 , -0.03636692, -0.03636424, -0.03635385, -0.03634824,\n",
      "        -0.03636343, -0.03635569, -0.03635214, -0.03636181, -0.03635452,\n",
      "        -0.03636863, -0.03634642, -0.0363576 , -0.03635689, -0.03635811,\n",
      "        -0.0363825 , -0.03637368, -0.03637143, -0.0363796 , -0.03635619,\n",
      "        -0.03636658, -0.03636869, -0.03645539]])\n",
      " array([[-0.05464822, -0.06299569, -0.06294559, -0.06299272, -0.06297136,\n",
      "        -0.06299155, -0.06298769, -0.06296666, -0.06298533, -0.06299122,\n",
      "        -0.06297556, -0.06297538, -0.06298243, -0.06296789],\n",
      "       [-0.06299569, -0.07284414, -0.07269353, -0.07273254, -0.07273328,\n",
      "        -0.07273379, -0.07271733, -0.07271305, -0.07274124, -0.07272978,\n",
      "        -0.07271773, -0.07272091, -0.07273765, -0.07271824],\n",
      "       [-0.06294559, -0.07269353, -0.07283386, -0.07272505, -0.07268835,\n",
      "        -0.07269318, -0.0727093 , -0.07270246, -0.07269667, -0.07267064,\n",
      "        -0.07270848, -0.07270899, -0.07270328, -0.07272015],\n",
      "       [-0.06299272, -0.07273254, -0.07272505, -0.07282917, -0.07271782,\n",
      "        -0.07273768, -0.07273574, -0.07271131, -0.07271717, -0.0727075 ,\n",
      "        -0.0727242 , -0.07272884, -0.07272356, -0.0727169 ],\n",
      "       [-0.06297136, -0.07273328, -0.07268835, -0.07271782, -0.07285113,\n",
      "        -0.07274218, -0.07270528, -0.07273205, -0.0726896 , -0.07272317,\n",
      "        -0.07270163, -0.07268909, -0.07271521, -0.07270608],\n",
      "       [-0.06299155, -0.07273379, -0.07269318, -0.07273768, -0.07274218,\n",
      "        -0.07283323, -0.07272845, -0.07273273, -0.07272501, -0.0727363 ,\n",
      "        -0.07271155, -0.07270753, -0.07271997, -0.07272209],\n",
      "       [-0.06298769, -0.07271733, -0.0727093 , -0.07273574, -0.07270528,\n",
      "        -0.07272845, -0.07282498, -0.07272902, -0.0727046 , -0.07270014,\n",
      "        -0.07270844, -0.07270893, -0.07272856, -0.07271317],\n",
      "       [-0.06296666, -0.07271305, -0.07270246, -0.07271131, -0.07273205,\n",
      "        -0.07273273, -0.07272902, -0.07282963, -0.07270567, -0.07272192,\n",
      "        -0.07271936, -0.07270924, -0.07271231, -0.07272421],\n",
      "       [-0.06298533, -0.07274124, -0.07269667, -0.07271717, -0.0726896 ,\n",
      "        -0.07272501, -0.0727046 , -0.07270567, -0.07284043, -0.07273396,\n",
      "        -0.07271083, -0.07272157, -0.07272057, -0.07271958],\n",
      "       [-0.06299122, -0.07272978, -0.07267064, -0.0727075 , -0.07272317,\n",
      "        -0.0727363 , -0.07270014, -0.07272192, -0.07273396, -0.07283251,\n",
      "        -0.07273121, -0.0727317 , -0.07272366, -0.07272679],\n",
      "       [-0.06297556, -0.07271773, -0.07270848, -0.0727242 , -0.07270163,\n",
      "        -0.07271155, -0.07270844, -0.07271936, -0.07271083, -0.07273121,\n",
      "        -0.07284354, -0.07274387, -0.07272981, -0.07274964],\n",
      "       [-0.06297538, -0.07272091, -0.07270899, -0.07272884, -0.07268909,\n",
      "        -0.07270753, -0.07270893, -0.07270924, -0.07272157, -0.0727317 ,\n",
      "        -0.07274387, -0.07284065, -0.07272293, -0.07274068],\n",
      "       [-0.06298243, -0.07273765, -0.07270328, -0.07272356, -0.07271521,\n",
      "        -0.07271997, -0.07272856, -0.07271231, -0.07272057, -0.07272366,\n",
      "        -0.07272981, -0.07272293, -0.07281953, -0.07272585],\n",
      "       [-0.06296789, -0.07271824, -0.07272015, -0.0727169 , -0.07270608,\n",
      "        -0.07272209, -0.07271317, -0.07272421, -0.07271958, -0.07272679,\n",
      "        -0.07274964, -0.07274068, -0.07272585, -0.0728256 ]])]\n",
      "(112, 112)\n",
      "(56, 56)\n",
      "(28, 28)\n",
      "(14, 14)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_laplacians))\n",
    "#print(test_laplacians[0])\n",
    "L = []\n",
    "for i in range(FLAGS.coarsening_levels):\n",
    "    L.append(train_laplacians[i][0])\n",
    "L = np.array(L)\n",
    "print(L)\n",
    "for i in range(4):\n",
    "    print(L[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib import models, graph, coarsening, utils\n",
    "common = {}\n",
    "common['dir_name']       = 'cifar10/'\n",
    "common['num_epochs']     = 2\n",
    "common['batch_size']     = 100\n",
    "common['decay_steps']    = (train_data.shape[0] + val_data.shape[0]) / common['batch_size']\n",
    "common['eval_frequency'] = 10 * common['num_epochs']\n",
    "common['brelu']          = 'b1relu'\n",
    "common['pool']           = 'mpool1'\n",
    "num_labels_per_image     = 2\n",
    "C = train_labels.shape[1]  # number of classes\n",
    "model_perf = utils.model_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 112\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 20\n",
      "    weights: M_0 * M_1 = 112 * 20 = 2240\n",
      "    biases: M_1 = 20\n",
      "step 20 / 100 (epoch 0.40 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 4.71e+00\n",
      "4416.0 / 7013 = 0.629687722801\n",
      "  validation precision: 0.43, recall : 0.42, f_measure: 0.42, mAP 0.57, MAP 0.69\n",
      "  time: 33s (wall 40s)\n",
      "step 40 / 100 (epoch 0.80 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 3.38e+00\n",
      "5089.0 / 7013 = 0.725652359903\n",
      "  validation precision: 0.54, recall : 0.61, f_measure: 0.57, mAP 0.82, MAP 0.81\n",
      "  time: 34s (wall 41s)\n",
      "step 60 / 100 (epoch 1.20 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 2.65e+00\n",
      "5341.0 / 7013 = 0.761585626693\n",
      "  validation precision: 0.52, recall : 0.67, f_measure: 0.59, mAP 0.88, MAP 0.84\n",
      "  time: 35s (wall 43s)\n",
      "step 80 / 100 (epoch 1.60 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 2.32e+00\n",
      "5425.0 / 7013 = 0.77356338229\n",
      "  validation precision: 0.54, recall : 0.69, f_measure: 0.61, mAP 0.90, MAP 0.85\n",
      "  time: 36s (wall 44s)\n",
      "step 100 / 100 (epoch 2.00 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 2.26e+00\n",
      "5468.0 / 7013 = 0.779694852417\n",
      "  validation precision: 0.55, recall : 0.71, f_measure: 0.62, mAP 0.91, MAP 0.86\n",
      "  time: 36s (wall 45s)\n",
      "validation peaks: precision = 0.55, recall = 0.71, f_measure = 0.6163989721157418, mAP = [ 0.91115499], MAP = [ 0.85926483]\n",
      "<class 'list'> <class 'list'>\n",
      "0 <class 'list'>\n",
      "0 <class 'list'>\n",
      "5905.0 / 7584 = 0.778612869198\n",
      "train precision: 0.58, recall : 0.71, f_measure: 0.64, mAP 0.91, MAP 0.88\n",
      "time: 1s (wall 1s)\n",
      "5468.0 / 7013 = 0.779694852417\n",
      "test  precision: 0.55, recall : 0.71, f_measure: 0.62, mAP 0.91, MAP 0.86\n",
      "time: 1s (wall 0s)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    name = 'softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['regularization'] = 5e-4\n",
    "    params['dropout']        = 1\n",
    "    params['learning_rate']  = 0.02\n",
    "    params['decay_rate']     = 0.95\n",
    "    params['momentum']       = 0.9\n",
    "    params['F']              = []\n",
    "    params['F_0']            = 20\n",
    "    params['K']              = []\n",
    "    params['p']              = []\n",
    "    params['M']              = [C] \n",
    "    params['train_laplacians'] = train_laplacians\n",
    "    params['test_laplacians'] = val_laplacians\n",
    "    params['val_laplacians'] = test_laplacians\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7584 7013\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_labels),np.sum(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common hyper-parameters for networks with one convolutional layer.\n",
    "common['regularization'] = 0\n",
    "common['dropout']        = 1\n",
    "common['learning_rate']  = 0.02\n",
    "common['decay_rate']     = 0.95\n",
    "common['momentum']       = 0.9\n",
    "common['F']              = [10]\n",
    "common['F_0']             = 20\n",
    "common['K']              = [100]\n",
    "common['p']              = [4]\n",
    "common['M']              = [C]\n",
    "common['train_laplacians'] = train_laplacians\n",
    "common['test_laplacians'] = val_laplacians\n",
    "common['val_laplacians'] = test_laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 4 = 280\n",
      "    weights: F_0 * F_1 * K_1 = 20 * 10 * 100 = 20000\n",
      "    biases: F_1 = 10\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 20\n",
      "    weights: M_1 * M_2 = 280 * 20 = 5600\n",
      "    biases: M_2 = 20\n",
      "0 th conv layer 100 112 112\n",
      "step 20 / 100 (epoch 0.40 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 4.16e+00\n",
      "3932.0 / 7013 = 0.560673035791\n",
      "  validation precision: 0.28, recall : 0.34, f_measure: 0.31, mAP 0.49, MAP 0.63\n",
      "  time: 22s (wall 25s)\n",
      "step 40 / 100 (epoch 0.80 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 3.47e+00\n",
      "4512.0 / 7013 = 0.64337658634\n",
      "  validation precision: 0.37, recall : 0.47, f_measure: 0.41, mAP 0.61, MAP 0.73\n",
      "  time: 28s (wall 31s)\n",
      "step 60 / 100 (epoch 1.20 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 2.85e+00\n",
      "4882.0 / 7013 = 0.696135747897\n",
      "  validation precision: 0.51, recall : 0.54, f_measure: 0.53, mAP 0.71, MAP 0.78\n",
      "  time: 33s (wall 36s)\n",
      "step 80 / 100 (epoch 1.60 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 2.65e+00\n",
      "5074.0 / 7013 = 0.723513474975\n",
      "  validation precision: 0.52, recall : 0.60, f_measure: 0.56, mAP 0.80, MAP 0.81\n",
      "  time: 39s (wall 42s)\n",
      "step 100 / 100 (epoch 2.00 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 2.51e+00\n",
      "5150.0 / 7013 = 0.734350491944\n",
      "  validation precision: 0.50, recall : 0.67, f_measure: 0.57, mAP 0.86, MAP 0.83\n",
      "  time: 44s (wall 48s)\n",
      "validation peaks: precision = 0.52, recall = 0.67, f_measure = 0.5721639738900325, mAP = [ 0.86474424], MAP = [ 0.83061064]\n",
      "<class 'list'> <class 'list'>\n",
      "1 <class 'list'>\n",
      "1 <class 'list'>\n",
      "5487.0 / 7584 = 0.723496835443\n",
      "train precision: 0.51, recall : 0.66, f_measure: 0.58, mAP 0.86, MAP 0.84\n",
      "time: 4s (wall 4s)\n",
      "5150.0 / 7013 = 0.734350491944\n",
      "test  precision: 0.50, recall : 0.67, f_measure: 0.57, mAP 0.86, MAP 0.83\n",
      "time: 4s (wall 4s)\n"
     ]
    }
   ],
   "source": [
    "# With 'chebyshev2' and 'b2relu', it corresponds to cgcnn2_2(L[0], F=10, K=20).\n",
    "if True:\n",
    "    name = 'cgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['filter'] = 'chebyshev5'\n",
    "#    params['filter'] = 'chebyshev2'\n",
    "#    params['brelu'] = 'b2relu'\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common hyper-parameters for LeNet5-like networks.\n",
    "common['regularization'] = 5e-4\n",
    "common['dropout']        = 0.5\n",
    "common['learning_rate']  = 0.02  # 0.03 in the paper but sgconv_sgconv_fc_softmax has difficulty to converge\n",
    "common['decay_rate']     = 0.95\n",
    "common['momentum']       = 0.9\n",
    "common['F']              = [32, 64]\n",
    "common['F_0']            = 20\n",
    "common['K']              = [25, 25]\n",
    "common['p']              = [2, 2]\n",
    "common['M']              = [512, C]\n",
    "common['train_laplacians'] = train_laplacians\n",
    "common['test_laplacians'] = val_laplacians\n",
    "common['val_laplacians'] = test_laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 32 / 2 = 1792\n",
      "    weights: F_0 * F_1 * K_1 = 20 * 32 * 25 = 16000\n",
      "    biases: F_1 = 32\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 64 / 2 = 1792\n",
      "    weights: F_1 * F_2 * K_2 = 32 * 64 * 25 = 51200\n",
      "    biases: F_2 = 64\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 512\n",
      "    weights: M_2 * M_3 = 1792 * 512 = 917504\n",
      "    biases: M_3 = 512\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 20\n",
      "    weights: M_3 * M_4 = 512 * 20 = 10240\n",
      "    biases: M_4 = 20\n",
      "0 th conv layer 100 112 112\n",
      "1 th conv layer 100 56 56\n",
      "step 20 / 100 (epoch 0.40 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 8.68e+00\n",
      "2426.0 / 7013 = 0.34592898902\n",
      "  validation precision: 0.07, recall : 0.10, f_measure: 0.09, mAP 0.10, MAP 0.40\n",
      "  time: 5s (wall 5s)\n",
      "step 40 / 100 (epoch 0.80 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 7.23e+00\n",
      "2424.0 / 7013 = 0.345643804363\n",
      "  validation precision: 0.02, recall : 0.10, f_measure: 0.04, mAP 0.10, MAP 0.38\n",
      "  time: 9s (wall 8s)\n",
      "step 60 / 100 (epoch 1.20 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 7.00e+00\n",
      "2752.0 / 7013 = 0.392414088122\n",
      "  validation precision: 0.07, recall : 0.10, f_measure: 0.08, mAP 0.11, MAP 0.44\n",
      "  time: 13s (wall 13s)\n",
      "step 80 / 100 (epoch 1.60 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 6.99e+00\n",
      "2772.0 / 7013 = 0.395265934693\n",
      "  validation precision: 0.07, recall : 0.11, f_measure: 0.08, mAP 0.12, MAP 0.44\n",
      "  time: 16s (wall 16s)\n",
      "step 100 / 100 (epoch 2.00 / 2):\n",
      "  learning_rate = 2.00e-02, loss_average = 6.94e+00\n",
      "3116.0 / 7013 = 0.444317695708\n",
      "  validation precision: 0.06, recall : 0.17, f_measure: 0.09, mAP 0.20, MAP 0.51\n",
      "  time: 20s (wall 20s)\n",
      "validation peaks: precision = 0.07, recall = 0.17, f_measure = 0.08968530948047206, mAP = [ 0.19519052], MAP = [ 0.51329222]\n",
      "<class 'list'> <class 'list'>\n",
      "2 <class 'list'>\n",
      "2 <class 'list'>\n",
      "3237.0 / 7584 = 0.426819620253\n",
      "train precision: 0.06, recall : 0.17, f_measure: 0.09, mAP 0.20, MAP 0.52\n",
      "time: 2s (wall 2s)\n",
      "3116.0 / 7013 = 0.444317695708\n",
      "test  precision: 0.06, recall : 0.17, f_measure: 0.09, mAP 0.20, MAP 0.51\n",
      "time: 2s (wall 2s)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    name = 'cgconv_cgconv_fc_softmax'  # 'Chebyshev'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
