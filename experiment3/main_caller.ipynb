{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from lib import models, graph, coarsening, utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.sparse\n",
    "from scipy.io import *\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Graphs.\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n",
    "# TODO: change cgcnn for combinatorial Laplacians.\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')\n",
    "\n",
    "# Directories.\n",
    "flags.DEFINE_string('dir_data', os.path.join('..', 'data', 'mnist'), 'Directory to store data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      " No Preprocessing Required\n"
     ]
    }
   ],
   "source": [
    "# Number of Nearest Neighbors\n",
    "k = 110\n",
    "\n",
    "base_storage_dir = '/data4/abhijeet/Datasets/PASCAL_VOC/GCN/preprocessing/experiment3/'\n",
    "temp_filename = base_storage_dir + 'scikit_fc7_k_' + str(k) + '.mat'\n",
    "\n",
    "if os.path.exists(temp_filename):\n",
    "    print('File already exists\\n No Preprocessing Required')\n",
    "else:\n",
    "    print('Do preprocessing --> TODO --> call the fucntion with a specific parameter')\n",
    "    \n",
    "data = scipy.io.loadmat( temp_filename)\n",
    "\n",
    "X_train          = data['X_train']\n",
    "X_test           = data['X_test']\n",
    "train_labels     = data['Y_train']\n",
    "test_labels      = data['Y_test']\n",
    "Adjacency_test   = data['Adjacency_test']\n",
    "Adjacency_train  = data['Adjacency_train']\n",
    "\n",
    "#delete unused variables\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#define laplacians, coarsen adjacency, permutate data points.\n",
    "train_laplacians = [[] for i in range(1 + FLAGS.coarsening_levels)]\n",
    "test_laplacians = [[] for i in range(1 + FLAGS.coarsening_levels)]\n",
    "\n",
    "train_perm ,test_perm = [] ,[]\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    graphs, temp_perm = coarsening.coarsen(Adjacency_train[0][i], levels=FLAGS.coarsening_levels, self_connections=False)\n",
    "    train_perm.append(temp_perm)\n",
    "    temp_L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    temp_L = [graph.rescale_L(A) for A in temp_L]\n",
    "    #graph.plot_spectrum(L)\n",
    "    temp_L = [A.toarray() for A in temp_L]\n",
    "    for j in range(1 + FLAGS.coarsening_levels):\n",
    "        train_laplacians[j].append(temp_L[j])     \n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    graphs, temp_perm = coarsening.coarsen(Adjacency_test[0][i], levels=FLAGS.coarsening_levels, self_connections=False)\n",
    "    test_perm.append(temp_perm)\n",
    "    temp_L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    temp_L = [graph.rescale_L(A) for A in temp_L]\n",
    "    #graph.plot_spectrum(L)\n",
    "    temp_L = [A.toarray() for A in temp_L]\n",
    "    for j in range(1 + FLAGS.coarsening_levels):\n",
    "        test_laplacians[j].append(temp_L[j])     \n",
    "\n",
    "#del Adjacency_test, Adjacency_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5\n",
      "4952\n",
      "(112, 112)\n",
      "4952\n",
      "(56, 56)\n",
      "4952\n",
      "(28, 28)\n",
      "4952\n",
      "(14, 14)\n"
     ]
    }
   ],
   "source": [
    "print(type(test_laplacians))\n",
    "print(len(test_laplacians))\n",
    "for i in range(4):\n",
    "    print(len(test_laplacians[i]))\n",
    "    print(test_laplacians[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 <class 'list'>\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_perm[0]),type(train_perm))\n",
    "train_data = np.zeros((X_train.shape[0], len(train_perm[0]), X_train.shape[2]))\n",
    "test_data = np.zeros((X_test.shape[0], len(test_perm[0]), X_test.shape[2]))\n",
    "for i in range(train_data.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    train_data[i,:,:] = coarsening.perm_data_point(X_train[i,:,:], train_perm[i])\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    test_data[i,:,:] = coarsening.perm_data_point(X_test[i,:,:], test_perm[i])\n",
    "\n",
    "val_data = test_data.copy()\n",
    "val_laplacians = test_laplacians.copy()\n",
    "val_labels = test_labels.copy()\n",
    "\n",
    "#del X_train, Y_train, X_test, Y_test \n",
    "#del perm_train, perm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[ array([[-0.00916256, -0.00910209, -0.00910398, ..., -0.00909108,\n",
      "        -0.00908656, -0.00909522],\n",
      "       [-0.00910209, -0.00916452, -0.00909317, ..., -0.00908922,\n",
      "        -0.00908687, -0.00909534],\n",
      "       [-0.00910398, -0.00909317, -0.00916536, ..., -0.00909255,\n",
      "        -0.00908611, -0.0090926 ],\n",
      "       ..., \n",
      "       [-0.00909108, -0.00908922, -0.00909255, ..., -0.00915947,\n",
      "        -0.00909905, -0.00909837],\n",
      "       [-0.00908656, -0.00908687, -0.00908611, ..., -0.00909905,\n",
      "        -0.00916364, -0.00908909],\n",
      "       [-0.00909522, -0.00909534, -0.0090926 , ..., -0.00909837,\n",
      "        -0.00908909, -0.00916216]])\n",
      " array([[-0.01826563, -0.01819792, -0.01819491, ..., -0.01817355,\n",
      "        -0.0181781 , -0.018182  ],\n",
      "       [-0.01819792, -0.01826408, -0.01818837, ..., -0.01816765,\n",
      "        -0.01817973, -0.01817587],\n",
      "       [-0.01819491, -0.01818837, -0.01826495, ..., -0.01817989,\n",
      "        -0.01818951, -0.01818773],\n",
      "       ..., \n",
      "       [-0.01817355, -0.01816765, -0.01817989, ..., -0.01825477,\n",
      "        -0.01818505, -0.01818436],\n",
      "       [-0.0181781 , -0.01817973, -0.01818951, ..., -0.01818505,\n",
      "        -0.01826563, -0.01819265],\n",
      "       [-0.018182  , -0.01817587, -0.01818773, ..., -0.01818436,\n",
      "        -0.01819265, -0.01825199]])\n",
      " array([[-0.03646277, -0.02572687, -0.03634497, -0.03637891, -0.03635991,\n",
      "        -0.03636784, -0.03637065, -0.03634618, -0.0363359 , -0.03636978,\n",
      "        -0.03635015, -0.0363649 , -0.03635204, -0.03634827, -0.03635779,\n",
      "        -0.036355  , -0.03634069, -0.03634937, -0.0363507 , -0.03636517,\n",
      "        -0.03636199, -0.03638647, -0.03636688, -0.03635986, -0.03634913,\n",
      "        -0.03635849, -0.03634916, -0.03635785],\n",
      "       [-0.02572687, -0.01826495, -0.02571732, -0.02571472, -0.02572557,\n",
      "        -0.02572041, -0.02572419, -0.02571667, -0.02569602, -0.02571867,\n",
      "        -0.02571201, -0.02571656, -0.02571189, -0.02570514, -0.02571262,\n",
      "        -0.02571648, -0.02571666, -0.02571445, -0.02571612, -0.02572237,\n",
      "        -0.02572487, -0.02572229, -0.02572371, -0.0257177 , -0.02571575,\n",
      "        -0.02571548, -0.02570803, -0.02572259],\n",
      "       [-0.03634497, -0.02571732, -0.03645233, -0.03637006, -0.03635471,\n",
      "        -0.03635624, -0.03634978, -0.03636625, -0.03634288, -0.03636116,\n",
      "        -0.03635112, -0.03636557, -0.03634844, -0.03635369, -0.03635025,\n",
      "        -0.0363696 , -0.03636342, -0.03637351, -0.03637153, -0.03636823,\n",
      "        -0.0363587 , -0.03636964, -0.03637462, -0.03637373, -0.03636551,\n",
      "        -0.03638087, -0.03637057, -0.03637403],\n",
      "       [-0.03637891, -0.02571472, -0.03637006, -0.03645618, -0.03635748,\n",
      "        -0.0363684 , -0.03635594, -0.03635291, -0.03634956, -0.03637786,\n",
      "        -0.03636094, -0.0363775 , -0.03634875, -0.03636406, -0.03635841,\n",
      "        -0.03636746, -0.03635987, -0.03636852, -0.03635631, -0.03636811,\n",
      "        -0.03636041, -0.03637415, -0.03637305, -0.03637563, -0.03636439,\n",
      "        -0.03637978, -0.03637199, -0.03637143],\n",
      "       [-0.03635991, -0.02572557, -0.03635471, -0.03635748, -0.03645893,\n",
      "        -0.03638789, -0.03635926, -0.03634693, -0.03634917, -0.0363584 ,\n",
      "        -0.0363499 , -0.03634803, -0.03634662, -0.03634309, -0.03633834,\n",
      "        -0.03634402, -0.03634889, -0.03634511, -0.03634483, -0.03636524,\n",
      "        -0.0363495 , -0.03634949, -0.03635071, -0.03635838, -0.03635065,\n",
      "        -0.03635417, -0.03633979, -0.03635014],\n",
      "       [-0.03636784, -0.02572041, -0.03635624, -0.0363684 , -0.03638789,\n",
      "        -0.03647065, -0.03636965, -0.03635929, -0.03634874, -0.03635875,\n",
      "        -0.03634423, -0.03634398, -0.03634056, -0.036352  , -0.03635097,\n",
      "        -0.03635592, -0.03634724, -0.03634027, -0.0363461 , -0.03636717,\n",
      "        -0.0363478 , -0.03635306, -0.0363564 , -0.03636138, -0.03635721,\n",
      "        -0.0363633 , -0.0363451 , -0.0363658 ],\n",
      "       [-0.03637065, -0.02572419, -0.03634978, -0.03635594, -0.03635926,\n",
      "        -0.03636965, -0.03645609, -0.03637241, -0.03635047, -0.03636361,\n",
      "        -0.03634604, -0.03634553, -0.03635805, -0.03636614, -0.03636893,\n",
      "        -0.03636391, -0.03635559, -0.03634762, -0.03634991, -0.03636108,\n",
      "        -0.03636899, -0.03636688, -0.03635275, -0.03635713, -0.03636379,\n",
      "        -0.03635678, -0.03635215, -0.03636396],\n",
      "       [-0.03634618, -0.02571667, -0.03636625, -0.03635291, -0.03634693,\n",
      "        -0.03635929, -0.03637241, -0.03646158, -0.03635719, -0.03637015,\n",
      "        -0.03634667, -0.03636783, -0.03636003, -0.03637406, -0.03636537,\n",
      "        -0.03638231, -0.03636071, -0.03636192, -0.03637918, -0.0363734 ,\n",
      "        -0.03635005, -0.03635821, -0.03635398, -0.03636413, -0.03636167,\n",
      "        -0.03636316, -0.03637623, -0.03637631],\n",
      "       [-0.0363359 , -0.02569602, -0.03634288, -0.03634956, -0.03634917,\n",
      "        -0.03634874, -0.03635047, -0.03635719, -0.03646069, -0.03636119,\n",
      "        -0.03636066, -0.03635564, -0.03635012, -0.03635602, -0.03635109,\n",
      "        -0.0363412 , -0.03635049, -0.03634544, -0.03633878, -0.03633728,\n",
      "        -0.0363317 , -0.03634016, -0.0363266 , -0.03634431, -0.03634904,\n",
      "        -0.036346  , -0.03634856, -0.03633276],\n",
      "       [-0.03636978, -0.02571867, -0.03636116, -0.03637786, -0.0363584 ,\n",
      "        -0.03635875, -0.03636361, -0.03637015, -0.03636119, -0.03644881,\n",
      "        -0.03636305, -0.03637831, -0.0363556 , -0.03637542, -0.03636949,\n",
      "        -0.03636601, -0.0363564 , -0.03636735, -0.03636952, -0.03636711,\n",
      "        -0.03636262, -0.03637969, -0.03636297, -0.03636154, -0.0363606 ,\n",
      "        -0.03636649, -0.03637366, -0.03636162],\n",
      "       [-0.03635015, -0.02571201, -0.03635112, -0.03636094, -0.0363499 ,\n",
      "        -0.03634423, -0.03634604, -0.03634667, -0.03636066, -0.03636305,\n",
      "        -0.03646825, -0.0363835 , -0.03636745, -0.0363597 , -0.03635299,\n",
      "        -0.03635113, -0.03634587, -0.03634948, -0.03633412, -0.03633947,\n",
      "        -0.03635418, -0.03635795, -0.03635053, -0.03636124, -0.03635998,\n",
      "        -0.03634541, -0.03635399, -0.0363464 ],\n",
      "       [-0.0363649 , -0.02571656, -0.03636557, -0.0363775 , -0.03634803,\n",
      "        -0.03634398, -0.03634553, -0.03636783, -0.03635564, -0.03637831,\n",
      "        -0.0363835 , -0.03645576, -0.03636088, -0.03637168, -0.03636371,\n",
      "        -0.03636973, -0.03635633, -0.03637109, -0.03636975, -0.03636429,\n",
      "        -0.03636222, -0.03637668, -0.03637035, -0.03637828, -0.03635548,\n",
      "        -0.0363581 , -0.03637167, -0.03637092],\n",
      "       [-0.03635204, -0.02571189, -0.03634844, -0.03634875, -0.03634662,\n",
      "        -0.03634056, -0.03635805, -0.03636003, -0.03635012, -0.0363556 ,\n",
      "        -0.03636745, -0.03636088, -0.03646091, -0.03637084, -0.03636288,\n",
      "        -0.03635927, -0.03634406, -0.03634514, -0.03634318, -0.0363312 ,\n",
      "        -0.03633939, -0.03634345, -0.03634841, -0.03634717, -0.03636514,\n",
      "        -0.03634244, -0.03636433, -0.03635382],\n",
      "       [-0.03634827, -0.02570514, -0.03635369, -0.03636406, -0.03634309,\n",
      "        -0.036352  , -0.03636614, -0.03637406, -0.03635602, -0.03637542,\n",
      "        -0.0363597 , -0.03637168, -0.03637084, -0.03646488, -0.03638958,\n",
      "        -0.03637807, -0.0363515 , -0.03636937, -0.03636328, -0.03635345,\n",
      "        -0.03634849, -0.0363536 , -0.03635291, -0.03636579, -0.03635937,\n",
      "        -0.03635503, -0.03637702, -0.03636549],\n",
      "       [-0.03635779, -0.02571262, -0.03635025, -0.03635841, -0.03633834,\n",
      "        -0.03635097, -0.03636893, -0.03636537, -0.03635109, -0.03636949,\n",
      "        -0.03635299, -0.03636371, -0.03636288, -0.03638958, -0.03645162,\n",
      "        -0.03637289, -0.03635569, -0.03635313, -0.03635729, -0.03634325,\n",
      "        -0.03634856, -0.03636772, -0.03635555, -0.03636168, -0.03636024,\n",
      "        -0.03634882, -0.03636942, -0.03636861],\n",
      "       [-0.036355  , -0.02571648, -0.0363696 , -0.03636746, -0.03634402,\n",
      "        -0.03635592, -0.03636391, -0.03638231, -0.0363412 , -0.03636601,\n",
      "        -0.03635113, -0.03636973, -0.03635927, -0.03637807, -0.03637289,\n",
      "        -0.03645865, -0.03636408, -0.03636983, -0.03637675, -0.03636706,\n",
      "        -0.03634922, -0.03636365, -0.03635968, -0.03636316, -0.03635878,\n",
      "        -0.03637857, -0.03638375, -0.0363869 ],\n",
      "       [-0.03634069, -0.02571666, -0.03636342, -0.03635987, -0.03634889,\n",
      "        -0.03634724, -0.03635559, -0.03636071, -0.03635049, -0.0363564 ,\n",
      "        -0.03634587, -0.03635633, -0.03634406, -0.0363515 , -0.03635569,\n",
      "        -0.03636408, -0.03644461, -0.03636964, -0.03635899, -0.03636053,\n",
      "        -0.03634419, -0.03635502, -0.03635255, -0.03635653, -0.03635326,\n",
      "        -0.0363573 , -0.03636065, -0.03635994],\n",
      "       [-0.03634937, -0.02571445, -0.03637351, -0.03636852, -0.03634511,\n",
      "        -0.03634027, -0.03634762, -0.03636192, -0.03634544, -0.03636735,\n",
      "        -0.03634948, -0.03637109, -0.03634514, -0.03636937, -0.03635313,\n",
      "        -0.03636983, -0.03636964, -0.0364575 , -0.03638044, -0.0363638 ,\n",
      "        -0.03635197, -0.03636092, -0.03636752, -0.03637046, -0.03634718,\n",
      "        -0.03636977, -0.03636651, -0.03635711],\n",
      "       [-0.0363507 , -0.02571612, -0.03637153, -0.03635631, -0.03634483,\n",
      "        -0.0363461 , -0.03634991, -0.03637918, -0.03633878, -0.03636952,\n",
      "        -0.03633412, -0.03636975, -0.03634318, -0.03636328, -0.03635729,\n",
      "        -0.03637675, -0.03635899, -0.03638044, -0.03645863, -0.03637586,\n",
      "        -0.03634999, -0.03636483, -0.03637918, -0.03636729, -0.03634351,\n",
      "        -0.03635947, -0.03637343, -0.03637438],\n",
      "       [-0.03636517, -0.02572237, -0.03636823, -0.03636811, -0.03636524,\n",
      "        -0.03636717, -0.03636108, -0.0363734 , -0.03633728, -0.03636711,\n",
      "        -0.03633947, -0.03636429, -0.0363312 , -0.03635345, -0.03634325,\n",
      "        -0.03636706, -0.03636053, -0.0363638 , -0.03637586, -0.03645769,\n",
      "        -0.03635409, -0.03636509, -0.03636187, -0.03637049, -0.03634286,\n",
      "        -0.03636338, -0.03636068, -0.03636724],\n",
      "       [-0.03636199, -0.02572487, -0.0363587 , -0.03636041, -0.0363495 ,\n",
      "        -0.0363478 , -0.03636899, -0.03635005, -0.0363317 , -0.03636262,\n",
      "        -0.03635418, -0.03636222, -0.03633939, -0.03634849, -0.03634856,\n",
      "        -0.03634922, -0.03634419, -0.03635197, -0.03634999, -0.03635409,\n",
      "        -0.03645308, -0.03637375, -0.03636652, -0.03636525, -0.0363544 ,\n",
      "        -0.0363539 , -0.0363416 , -0.0363617 ],\n",
      "       [-0.03638647, -0.02572229, -0.03636964, -0.03637415, -0.03634949,\n",
      "        -0.03635306, -0.03636688, -0.03635821, -0.03634016, -0.03637969,\n",
      "        -0.03635795, -0.03637668, -0.03634345, -0.0363536 , -0.03636772,\n",
      "        -0.03636365, -0.03635502, -0.03636092, -0.03636483, -0.03636509,\n",
      "        -0.03637375, -0.03646179, -0.03638148, -0.03637135, -0.03635969,\n",
      "        -0.03636848, -0.03636096, -0.03636669],\n",
      "       [-0.03636688, -0.02572371, -0.03637462, -0.03637305, -0.03635071,\n",
      "        -0.0363564 , -0.03635275, -0.03635398, -0.0363266 , -0.03636297,\n",
      "        -0.03635053, -0.03637035, -0.03634841, -0.03635291, -0.03635555,\n",
      "        -0.03635968, -0.03635255, -0.03636752, -0.03637918, -0.03636187,\n",
      "        -0.03636652, -0.03638148, -0.03645516, -0.03637562, -0.03635948,\n",
      "        -0.03636626, -0.03636085, -0.03637441],\n",
      "       [-0.03635986, -0.0257177 , -0.03637373, -0.03637563, -0.03635838,\n",
      "        -0.03636138, -0.03635713, -0.03636413, -0.03634431, -0.03636154,\n",
      "        -0.03636124, -0.03637828, -0.03634717, -0.03636579, -0.03636168,\n",
      "        -0.03636316, -0.03635653, -0.03637046, -0.03636729, -0.03637049,\n",
      "        -0.03636525, -0.03637135, -0.03637562, -0.03645072, -0.03636625,\n",
      "        -0.03636927, -0.03636022, -0.03637326],\n",
      "       [-0.03634913, -0.02571575, -0.03636551, -0.03636439, -0.03635065,\n",
      "        -0.03635721, -0.03636379, -0.03636167, -0.03634904, -0.0363606 ,\n",
      "        -0.03635998, -0.03635548, -0.03636514, -0.03635937, -0.03636024,\n",
      "        -0.03635878, -0.03635326, -0.03634718, -0.03634351, -0.03634286,\n",
      "        -0.0363544 , -0.03635969, -0.03635948, -0.03636625, -0.03644959,\n",
      "        -0.03636956, -0.03635618, -0.03636596],\n",
      "       [-0.03635849, -0.02571548, -0.03638087, -0.03637978, -0.03635417,\n",
      "        -0.0363633 , -0.03635678, -0.03636316, -0.036346  , -0.03636649,\n",
      "        -0.03634541, -0.0363581 , -0.03634244, -0.03635503, -0.03634882,\n",
      "        -0.03637857, -0.0363573 , -0.03636977, -0.03635947, -0.03636338,\n",
      "        -0.0363539 , -0.03636848, -0.03636626, -0.03636927, -0.03636956,\n",
      "        -0.03646233, -0.03637186, -0.03636532],\n",
      "       [-0.03634916, -0.02570803, -0.03637057, -0.03637199, -0.03633979,\n",
      "        -0.0363451 , -0.03635215, -0.03637623, -0.03634856, -0.03637366,\n",
      "        -0.03635399, -0.03637167, -0.03636433, -0.03637702, -0.03636942,\n",
      "        -0.03638375, -0.03636065, -0.03636651, -0.03637343, -0.03636068,\n",
      "        -0.0363416 , -0.03636096, -0.03636085, -0.03636022, -0.03635618,\n",
      "        -0.03637186, -0.03644778, -0.03637307],\n",
      "       [-0.03635785, -0.02572259, -0.03637403, -0.03637143, -0.03635014,\n",
      "        -0.0363658 , -0.03636396, -0.03637631, -0.03633276, -0.03636162,\n",
      "        -0.0363464 , -0.03637092, -0.03635382, -0.03636549, -0.03636861,\n",
      "        -0.0363869 , -0.03635994, -0.03635711, -0.03637438, -0.03636724,\n",
      "        -0.0363617 , -0.03636669, -0.03637441, -0.03637326, -0.03636596,\n",
      "        -0.03636532, -0.03637307, -0.03645146]])\n",
      " array([[-0.05465235, -0.0629842 , -0.06299212, -0.06298373, -0.06296661,\n",
      "        -0.06297768, -0.06296446, -0.06297659, -0.06296428, -0.06298221,\n",
      "        -0.06300456, -0.06298967, -0.06297447, -0.06297387],\n",
      "       [-0.0629842 , -0.07282431, -0.07271841, -0.07271244, -0.07271573,\n",
      "        -0.07272756, -0.07270747, -0.07272286, -0.07273266, -0.07273209,\n",
      "        -0.07273145, -0.07274852, -0.07274528, -0.07274401],\n",
      "       [-0.06299212, -0.07271841, -0.07285268, -0.07271757, -0.07270753,\n",
      "        -0.07269307, -0.07269114, -0.07269463, -0.07269076, -0.07271167,\n",
      "        -0.07269993, -0.07271344, -0.07271267, -0.07270042],\n",
      "       [-0.06298373, -0.07271244, -0.07271757, -0.07283124, -0.07272071,\n",
      "        -0.07270303, -0.07272914, -0.07274026, -0.07271292, -0.07273178,\n",
      "        -0.07272206, -0.07271399, -0.0727227 , -0.07273433],\n",
      "       [-0.06296661, -0.07271573, -0.07270753, -0.07272071, -0.07281593,\n",
      "        -0.07272883, -0.07271858, -0.0727139 , -0.07270984, -0.07270635,\n",
      "        -0.07270709, -0.07269771, -0.07271107, -0.0727083 ],\n",
      "       [-0.06297768, -0.07272756, -0.07269307, -0.07270303, -0.07272883,\n",
      "        -0.0728455 , -0.07272985, -0.07271879, -0.07271139, -0.07270382,\n",
      "        -0.07272552, -0.0727302 , -0.07270948, -0.07272149],\n",
      "       [-0.06296446, -0.07270747, -0.07269114, -0.07272914, -0.07271858,\n",
      "        -0.07272985, -0.07283374, -0.0727449 , -0.07270504, -0.07269555,\n",
      "        -0.07269247, -0.07270714, -0.07271098, -0.07273034],\n",
      "       [-0.06297659, -0.07272286, -0.07269463, -0.07274026, -0.0727139 ,\n",
      "        -0.07271879, -0.0727449 , -0.07282803, -0.07272136, -0.07272218,\n",
      "        -0.07271458, -0.07272004, -0.0727232 , -0.07275434],\n",
      "       [-0.06296428, -0.07273266, -0.07269076, -0.07271292, -0.07270984,\n",
      "        -0.07271139, -0.07270504, -0.07272136, -0.0728207 , -0.07273188,\n",
      "        -0.07270605, -0.07272353, -0.07271375, -0.07272211],\n",
      "       [-0.06298221, -0.07273209, -0.07271167, -0.07273178, -0.07270635,\n",
      "        -0.07270382, -0.07269555, -0.07272218, -0.07273188, -0.07283402,\n",
      "        -0.072717  , -0.07273941, -0.07270461, -0.07273786],\n",
      "       [-0.06300456, -0.07273145, -0.07269993, -0.07272206, -0.07270709,\n",
      "        -0.07272552, -0.07269247, -0.07271458, -0.07270605, -0.072717  ,\n",
      "        -0.07283119, -0.0727423 , -0.07271824, -0.07271548],\n",
      "       [-0.06298967, -0.07274852, -0.07271344, -0.07271399, -0.07269771,\n",
      "        -0.0727302 , -0.07270714, -0.07272004, -0.07272353, -0.07273941,\n",
      "        -0.0727423 , -0.07282856, -0.07273063, -0.07273437],\n",
      "       [-0.06297447, -0.07274528, -0.07271267, -0.0727227 , -0.07271107,\n",
      "        -0.07270948, -0.07271098, -0.0727232 , -0.07271375, -0.07270461,\n",
      "        -0.07271824, -0.07273063, -0.07282552, -0.07272966],\n",
      "       [-0.06297387, -0.07274401, -0.07270042, -0.07273433, -0.0727083 ,\n",
      "        -0.07272149, -0.07273034, -0.07275434, -0.07272211, -0.07273786,\n",
      "        -0.07271548, -0.07273437, -0.07272966, -0.07282269]])]\n",
      "(112, 112)\n",
      "(56, 56)\n",
      "(28, 28)\n",
      "(14, 14)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_laplacians))\n",
    "#print(test_laplacians[0])\n",
    "L = []\n",
    "for i in range(FLAGS.coarsening_levels):\n",
    "    L.append(train_laplacians[i][0])\n",
    "L = np.array(L)\n",
    "print(L)\n",
    "for i in range(4):\n",
    "    print(L[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib import models, graph, coarsening, utils\n",
    "common = {}\n",
    "common['dir_name']       = 'cifar10/'\n",
    "common['num_epochs']     = 20\n",
    "common['batch_size']     = 100\n",
    "common['decay_steps']    = (train_data.shape[0] + val_data.shape[0]) / common['batch_size']\n",
    "common['eval_frequency'] = 10 * common['num_epochs']\n",
    "common['brelu']          = 'b1relu'\n",
    "common['pool']           = 'mpool1'\n",
    "num_labels_per_image     = 2\n",
    "C = train_labels.shape[1]  # number of classes\n",
    "model_perf = utils.model_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 112\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 20\n",
      "    weights: M_0 * M_1 = 112 * 20 = 2240\n",
      "    biases: M_1 = 20\n",
      "step 2000 / 10022 (epoch 39.91 / 200):\n",
      "  learning_rate = 7.17e-03, loss_average = 1.00e+00\n",
      "5595.0 / 7013 = 0.797804078141\n",
      "  validation precision: 0.57, recall : 0.75, f_measure: 0.65, mAP 0.93, MAP 0.88\n",
      "  time: 30s (wall 31s)\n",
      "step 4000 / 10022 (epoch 79.82 / 200):\n",
      "  learning_rate = 2.57e-03, loss_average = 8.98e-01\n",
      "5573.0 / 7013 = 0.794667046913\n",
      "  validation precision: 0.57, recall : 0.75, f_measure: 0.64, mAP 0.93, MAP 0.88\n",
      "  time: 44s (wall 40s)\n",
      "step 6000 / 10022 (epoch 119.74 / 200):\n",
      "  learning_rate = 9.21e-04, loss_average = 8.60e-01\n",
      "5579.0 / 7013 = 0.795522600884\n",
      "  validation precision: 0.57, recall : 0.75, f_measure: 0.64, mAP 0.93, MAP 0.88\n",
      "  time: 58s (wall 49s)\n",
      "step 8000 / 10022 (epoch 159.65 / 200):\n",
      "  learning_rate = 3.30e-04, loss_average = 8.51e-01\n",
      "5578.0 / 7013 = 0.795380008556\n",
      "  validation precision: 0.57, recall : 0.75, f_measure: 0.64, mAP 0.93, MAP 0.88\n",
      "  time: 72s (wall 59s)\n",
      "step 10000 / 10022 (epoch 199.56 / 200):\n",
      "  learning_rate = 1.18e-04, loss_average = 8.43e-01\n",
      "5577.0 / 7013 = 0.795237416227\n",
      "  validation precision: 0.57, recall : 0.75, f_measure: 0.64, mAP 0.93, MAP 0.88\n",
      "  time: 86s (wall 68s)\n",
      "step 10022 / 10022 (epoch 200.00 / 200):\n",
      "  learning_rate = 1.18e-04, loss_average = 8.48e-01\n",
      "5577.0 / 7013 = 0.795237416227\n",
      "  validation precision: 0.57, recall : 0.75, f_measure: 0.64, mAP 0.93, MAP 0.88\n",
      "  time: 87s (wall 69s)\n",
      "validation peaks: precision = 0.57, recall = 0.75, f_measure = 0.6462397416130387, mAP = [ 0.93479675], MAP = [ 0.87780532]\n",
      "<class 'list'> <class 'list'>\n",
      "6993.0 / 7584 = 0.92207278481\n",
      "train precision: 0.73, recall : 0.91, f_measure: 0.81, mAP 0.97, MAP 0.98\n",
      "time: 1s (wall 1s)\n",
      "5577.0 / 7013 = 0.795237416227\n",
      "test  precision: 0.57, recall : 0.75, f_measure: 0.64, mAP 0.93, MAP 0.88\n",
      "time: 1s (wall 0s)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    name = 'softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['regularization'] = 5e-4\n",
    "    params['dropout']        = 1\n",
    "    params['learning_rate']  = 0.02\n",
    "    params['decay_rate']     = 0.95\n",
    "    params['momentum']       = 0.9\n",
    "    params['F']              = []\n",
    "    params['F_0']            = 20\n",
    "    params['K']              = []\n",
    "    params['p']              = []\n",
    "    params['M']              = [C] \n",
    "    params['train_laplacians'] = train_laplacians\n",
    "    params['test_laplacians'] = val_laplacians\n",
    "    params['val_laplacians'] = test_laplacians\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7584 7013\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_labels),np.sum(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common hyper-parameters for networks with one convolutional layer.\n",
    "common['regularization'] = 0\n",
    "common['dropout']        = 1\n",
    "common['learning_rate']  = 0.02\n",
    "common['decay_rate']     = 0.95\n",
    "common['momentum']       = 0.9\n",
    "common['F']              = [10]\n",
    "common['F_0']            = 20\n",
    "common['K']              = [100]\n",
    "common['p']              = [4]\n",
    "common['M']              = [C]\n",
    "common['train_laplacians'] = train_laplacians\n",
    "common['test_laplacians'] = val_laplacians\n",
    "common['val_laplacians'] = test_laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 4 = 280\n",
      "    weights: F_0 * F_1 * K_1 = 20 * 10 * 100 = 20000\n",
      "    biases: F_1 = 10\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 20\n",
      "    weights: M_1 * M_2 = 280 * 20 = 5600\n",
      "    biases: M_2 = 20\n",
      "0 th conv layer 100 112 112\n",
      "step 2000 / 10022 (epoch 39.91 / 200):\n",
      "  learning_rate = 7.17e-03, loss_average = 1.74e+00\n",
      "5314.0 / 7013 = 0.757735633823\n",
      "  validation precision: 0.53, recall : 0.69, f_measure: 0.60, mAP 0.91, MAP 0.85\n",
      "  time: 114s (wall 220s)\n",
      "step 4000 / 10022 (epoch 79.82 / 200):\n",
      "  learning_rate = 2.57e-03, loss_average = 1.51e+00\n",
      "5299.0 / 7013 = 0.755596748895\n",
      "  validation precision: 0.52, recall : 0.70, f_measure: 0.60, mAP 0.92, MAP 0.84\n",
      "  time: 192s (wall 395s)\n",
      "step 6000 / 10022 (epoch 119.74 / 200):\n",
      "  learning_rate = 9.21e-04, loss_average = 1.53e+00\n",
      "5326.0 / 7013 = 0.759446741765\n",
      "  validation precision: 0.52, recall : 0.70, f_measure: 0.60, mAP 0.92, MAP 0.85\n",
      "  time: 266s (wall 568s)\n",
      "step 8000 / 10022 (epoch 159.65 / 200):\n",
      "  learning_rate = 3.30e-04, loss_average = 1.50e+00\n",
      "5319.0 / 7013 = 0.758448595466\n",
      "  validation precision: 0.52, recall : 0.70, f_measure: 0.60, mAP 0.92, MAP 0.85\n",
      "  time: 343s (wall 743s)\n",
      "step 10000 / 10022 (epoch 199.56 / 200):\n",
      "  learning_rate = 1.18e-04, loss_average = 1.44e+00\n",
      "5318.0 / 7013 = 0.758306003137\n",
      "  validation precision: 0.52, recall : 0.70, f_measure: 0.60, mAP 0.92, MAP 0.85\n",
      "  time: 420s (wall 918s)\n",
      "step 10022 / 10022 (epoch 200.00 / 200):\n",
      "  learning_rate = 1.18e-04, loss_average = 1.49e+00\n",
      "5314.0 / 7013 = 0.757735633823\n",
      "  validation precision: 0.52, recall : 0.70, f_measure: 0.60, mAP 0.91, MAP 0.85\n",
      "  time: 427s (wall 925s)\n",
      "validation peaks: precision = 0.53, recall = 0.70, f_measure = 0.6007882343032077, mAP = [ 0.91778981], MAP = [ 0.84823954]\n",
      "<class 'list'> <class 'list'>\n",
      "6311.0 / 7584 = 0.832146624473\n",
      "train precision: 0.65, recall : 0.80, f_measure: 0.72, mAP 0.95, MAP 0.93\n",
      "time: 5s (wall 5s)\n",
      "5314.0 / 7013 = 0.757735633823\n",
      "test  precision: 0.52, recall : 0.70, f_measure: 0.60, mAP 0.91, MAP 0.85\n",
      "time: 5s (wall 5s)\n"
     ]
    }
   ],
   "source": [
    "# With 'chebyshev2' and 'b2relu', it corresponds to cgcnn2_2(L[0], F=10, K=20).\n",
    "if True:\n",
    "    name = 'cgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['filter'] = 'chebyshev5'\n",
    "#    params['filter'] = 'chebyshev2'\n",
    "#    params['brelu'] = 'b2relu'\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common hyper-parameters for LeNet5-like networks.\n",
    "common['regularization'] = 5e-4\n",
    "common['dropout']        = 0.5\n",
    "common['learning_rate']  = 0.02  # 0.03 in the paper but sgconv_sgconv_fc_softmax has difficulty to converge\n",
    "common['decay_rate']     = 0.995\n",
    "common['momentum']       = 0.9\n",
    "common['F']              = [20,40,60]#[32, 64]\n",
    "common['F_0']            = 20\n",
    "common['K']              = [15,15,15]#[25, 25]\n",
    "common['p']              = [2, 2, 2] #[2,2]\n",
    "common['M']              = [C]#[512, C]\n",
    "common['train_laplacians'] = train_laplacians\n",
    "common['test_laplacians'] = val_laplacians\n",
    "common['val_laplacians'] = test_laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 20 / 2 = 1120\n",
      "    weights: F_0 * F_1 * K_1 = 20 * 20 * 15 = 6000\n",
      "    biases: F_1 = 20\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 40 / 2 = 1120\n",
      "    weights: F_1 * F_2 * K_2 = 20 * 40 * 15 = 12000\n",
      "    biases: F_2 = 40\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 60 / 2 = 840\n",
      "    weights: F_2 * F_3 * K_3 = 40 * 60 * 15 = 36000\n",
      "    biases: F_3 = 60\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 20\n",
      "    weights: M_3 * M_4 = 840 * 20 = 16800\n",
      "    biases: M_4 = 20\n",
      "0 th conv layer 100 112 112\n",
      "1 th conv layer 100 56 56\n",
      "2 th conv layer 100 28 28\n",
      "step 2000 / 10022 (epoch 39.91 / 200):\n",
      "  learning_rate = 1.91e-28, loss_average = 2.39e+00\n",
      "5107.0 / 7013 = 0.728219021817\n",
      "  validation precision: 0.50, recall : 0.62, f_measure: 0.55, mAP 0.80, MAP 0.82\n",
      "  time: 67s (wall 57s)\n",
      "step 4000 / 10022 (epoch 79.82 / 200):\n",
      "  learning_rate = 0.00e+00, loss_average = 2.44e+00\n",
      "5107.0 / 7013 = 0.728219021817\n",
      "  validation precision: 0.50, recall : 0.62, f_measure: 0.55, mAP 0.80, MAP 0.82\n",
      "  time: 131s (wall 113s)\n",
      "step 6000 / 10022 (epoch 119.74 / 200):\n",
      "  learning_rate = 0.00e+00, loss_average = 2.44e+00\n",
      "5107.0 / 7013 = 0.728219021817\n",
      "  validation precision: 0.50, recall : 0.62, f_measure: 0.55, mAP 0.80, MAP 0.82\n",
      "  time: 191s (wall 167s)\n",
      "step 8000 / 10022 (epoch 159.65 / 200):\n",
      "  learning_rate = 0.00e+00, loss_average = 2.42e+00\n",
      "5107.0 / 7013 = 0.728219021817\n",
      "  validation precision: 0.50, recall : 0.62, f_measure: 0.55, mAP 0.80, MAP 0.82\n",
      "  time: 251s (wall 220s)\n",
      "step 10000 / 10022 (epoch 199.56 / 200):\n",
      "  learning_rate = 0.00e+00, loss_average = 2.45e+00\n",
      "5107.0 / 7013 = 0.728219021817\n",
      "  validation precision: 0.50, recall : 0.62, f_measure: 0.55, mAP 0.80, MAP 0.82\n",
      "  time: 315s (wall 276s)\n",
      "step 10022 / 10022 (epoch 200.00 / 200):\n",
      "  learning_rate = 0.00e+00, loss_average = 2.41e+00\n",
      "5107.0 / 7013 = 0.728219021817\n",
      "  validation precision: 0.50, recall : 0.62, f_measure: 0.55, mAP 0.80, MAP 0.82\n",
      "  time: 319s (wall 280s)\n",
      "validation peaks: precision = 0.50, recall = 0.62, f_measure = 0.5533669578292727, mAP = [ 0.79629724], MAP = [ 0.81573865]\n",
      "<class 'list'> <class 'list'>\n",
      "5400.0 / 7584 = 0.712025316456\n",
      "train precision: 0.52, recall : 0.61, f_measure: 0.56, mAP 0.80, MAP 0.83\n",
      "time: 2s (wall 2s)\n",
      "5107.0 / 7013 = 0.728219021817\n",
      "test  precision: 0.50, recall : 0.62, f_measure: 0.55, mAP 0.80, MAP 0.82\n",
      "time: 2s (wall 2s)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    name = 'cgconv_cgconv_fc_softmax'  # 'Chebyshev'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 20 / 2 = 1120\n",
      "    weights: F_0 * F_1 * K_1 = 20 * 20 * 15 = 6000\n",
      "    biases: F_1 = 20\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 40 / 2 = 1120\n",
      "    weights: F_1 * F_2 * K_2 = 20 * 40 * 15 = 12000\n",
      "    biases: F_2 = 40\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 60 / 2 = 840\n",
      "    weights: F_2 * F_3 * K_3 = 40 * 60 * 15 = 36000\n",
      "    biases: F_3 = 60\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 20\n",
      "    weights: M_3 * M_4 = 840 * 20 = 16800\n",
      "    biases: M_4 = 20\n",
      "0 th conv layer 100 112 112\n",
      "1 th conv layer 100 56 56\n",
      "2 th conv layer 100 28 28\n",
      "step 200 / 1002 (epoch 3.99 / 20):\n",
      "  learning_rate = 1.00e-03, loss_average = 2.58e+00\n",
      "4934.0 / 7013 = 0.70355054898\n",
      "  validation precision: 0.41, recall : 0.55, f_measure: 0.47, mAP 0.67, MAP 0.78\n",
      "  time: 11s (wall 9s)\n",
      "step 400 / 1002 (epoch 7.98 / 20):\n",
      "  learning_rate = 1.25e-07, loss_average = 2.62e+00\n",
      "4947.0 / 7013 = 0.705404249251\n",
      "  validation precision: 0.42, recall : 0.55, f_measure: 0.48, mAP 0.68, MAP 0.78\n",
      "  time: 20s (wall 17s)\n",
      "step 600 / 1002 (epoch 11.97 / 20):\n",
      "  learning_rate = 3.13e-10, loss_average = 2.63e+00\n",
      "4946.0 / 7013 = 0.705261656923\n",
      "  validation precision: 0.42, recall : 0.55, f_measure: 0.48, mAP 0.68, MAP 0.78\n",
      "  time: 29s (wall 25s)\n",
      "step 800 / 1002 (epoch 15.96 / 20):\n",
      "  learning_rate = 7.81e-13, loss_average = 2.57e+00\n",
      "4946.0 / 7013 = 0.705261656923\n",
      "  validation precision: 0.42, recall : 0.55, f_measure: 0.48, mAP 0.68, MAP 0.78\n",
      "  time: 37s (wall 33s)\n",
      "step 1000 / 1002 (epoch 19.96 / 20):\n",
      "  learning_rate = 1.95e-15, loss_average = 2.59e+00\n",
      "4946.0 / 7013 = 0.705261656923\n",
      "  validation precision: 0.42, recall : 0.55, f_measure: 0.48, mAP 0.68, MAP 0.78\n",
      "  time: 46s (wall 41s)\n",
      "step 1002 / 1002 (epoch 20.00 / 20):\n",
      "  learning_rate = 1.95e-15, loss_average = 2.58e+00\n",
      "4946.0 / 7013 = 0.705261656923\n",
      "  validation precision: 0.42, recall : 0.55, f_measure: 0.48, mAP 0.68, MAP 0.78\n",
      "  time: 49s (wall 44s)\n",
      "validation peaks: precision = 0.42, recall = 0.55, f_measure = 0.4786069785548233, mAP = [ 0.68162914], MAP = [ 0.78355047]\n",
      "<class 'list'> <class 'list'>\n",
      "5246.0 / 7584 = 0.691719409283\n",
      "train precision: 0.42, recall : 0.55, f_measure: 0.48, mAP 0.67, MAP 0.79\n",
      "time: 2s (wall 2s)\n",
      "4946.0 / 7013 = 0.705261656923\n",
      "test  precision: 0.42, recall : 0.55, f_measure: 0.48, mAP 0.68, MAP 0.78\n",
      "time: 2s (wall 2s)\n"
     ]
    }
   ],
   "source": [
    "# Common hyper-parameters for LeNet5-like networks.\n",
    "common['regularization'] = 5e-4\n",
    "common['dropout']        = 0.5\n",
    "common['learning_rate']  = 0.02  # 0.03 in the paper but sgconv_sgconv_fc_softmax has difficulty to converge\n",
    "common['decay_rate']     = 0.95\n",
    "common['momentum']       = 0.9\n",
    "common['F']              = [20,40,60]#[32, 64]\n",
    "common['F_0']            = 20\n",
    "common['K']              = [15,15,15]#[25, 25]\n",
    "common['p']              = [2, 2, 2] #[2,2]\n",
    "common['M']              = [C]#[512, C]\n",
    "common['train_laplacians'] = train_laplacians\n",
    "common['test_laplacians'] = val_laplacians\n",
    "common['val_laplacians'] = test_laplacians\n",
    "if True:\n",
    "    name = 'cgconv_cgconv_fc_softmax'  # 'Chebyshev'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
