{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Laplacians for Pascal-VOC with only fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from lib import models, graph, coarsening, utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse\n",
    "from scipy.io import *\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Graphs.\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n",
    "# TODO: change cgcnn for combinatorial Laplacians.\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')\n",
    "\n",
    "# Directories.\n",
    "flags.DEFINE_string('dir_data', os.path.join('..', 'data', 'mnist'), 'Directory to store data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Nearest Neighbors\n",
    "k = 110\n",
    "\n",
    "base_storage_dir = '/data4/abhijeet/Datasets/PASCAL_VOC/GCN/preprocessing/experiment3/'\n",
    "temp_filename = base_storage_dir + 'scikit_fc7_k_' + str(k) + '.mat'\n",
    "\n",
    "if os.path.exists(temp_filename):\n",
    "    print('File already exists\\n No Preprocessing Required')\n",
    "else:\n",
    "    print('Do preprocessing --> TODO --> call the fucntion with a specific parameter')\n",
    "    \n",
    "data = scipy.io.loadmat( temp_filename)\n",
    "\n",
    "X_train          = data['X_train']\n",
    "X_test           = data['X_test']\n",
    "train_labels     = data['Y_train']\n",
    "test_labels      = data['Y_test']\n",
    "Adjacency_test   = data['Adjacency_test']\n",
    "Adjacency_train  = data['Adjacency_train']\n",
    "\n",
    "#delete unused variables\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define laplacians, coarsen adjacency, permutate data points.\n",
    "train_laplacians = [[] for i in range(1 + FLAGS.coarsening_levels)]\n",
    "test_laplacians = [[] for i in range(1 + FLAGS.coarsening_levels)]\n",
    "\n",
    "train_perm ,test_perm = [] ,[]\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    garbage_adjacency = csr_matrix(np.random.rand(k,k))\n",
    "    graphs, temp_perm = coarsening.coarsen(garbage_adjacency, levels=FLAGS.coarsening_levels, self_connections=False)\n",
    "    train_perm.append(temp_perm)\n",
    "    temp_L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    temp_L = [graph.rescale_L(A) for A in temp_L]\n",
    "    #graph.plot_spectrum(L)\n",
    "    temp_L = [A.toarray() for A in temp_L]\n",
    "    for j in range(1 + FLAGS.coarsening_levels):\n",
    "        train_laplacians[j].append(temp_L[j])     \n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    garbage_adjacency = csr_matrix(np.random.rand(k,k))\n",
    "    graphs, temp_perm = coarsening.coarsen(garbage_adjacency, levels=FLAGS.coarsening_levels, self_connections=False)\n",
    "    test_perm.append(temp_perm)\n",
    "    temp_L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    temp_L = [graph.rescale_L(A) for A in temp_L]\n",
    "    #graph.plot_spectrum(L)\n",
    "    temp_L = [A.toarray() for A in temp_L]\n",
    "    for j in range(1 + FLAGS.coarsening_levels):\n",
    "        test_laplacians[j].append(temp_L[j])     \n",
    "\n",
    "#del Adjacency_test, Adjacency_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(train_perm[0]),type(train_perm))\n",
    "train_data = np.zeros((X_train.shape[0], len(train_perm[0]), X_train.shape[2]))\n",
    "test_data = np.zeros((X_test.shape[0], len(test_perm[0]), X_test.shape[2]))\n",
    "for i in range(train_data.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    train_data[i,:,:] = coarsening.perm_data_point(X_train[i,:,:], train_perm[i])\n",
    "\n",
    "for i in range(test_data.shape[0]):\n",
    "    if not(i%1000):\n",
    "        print(i)\n",
    "    test_data[i,:,:] = coarsening.perm_data_point(X_test[i,:,:], test_perm[i])\n",
    "\n",
    "val_data = test_data.copy()\n",
    "val_laplacians = test_laplacians.copy()\n",
    "val_labels = test_labels.copy()\n",
    "\n",
    "#del X_train, Y_train, X_test, Y_test \n",
    "#del perm_train, perm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(test_laplacians))\n",
    "#print(test_laplacians[0])\n",
    "L = []\n",
    "for i in range(FLAGS.coarsening_levels):\n",
    "    L.append(train_laplacians[i][0])\n",
    "L = np.array(L)\n",
    "for i in range(4):\n",
    "    print(L[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib import models, graph, coarsening, utils\n",
    "common = {}\n",
    "common['dir_name']       = 'cifar10/'\n",
    "common['num_epochs']     = 200\n",
    "common['batch_size']     = 100\n",
    "common['decay_steps']    = (train_data.shape[0] + val_data.shape[0]) / common['batch_size']\n",
    "common['eval_frequency'] = 10 * common['num_epochs']\n",
    "common['brelu']          = 'b1relu'\n",
    "common['pool']           = 'mpool1'\n",
    "num_labels_per_image     = 2\n",
    "C = train_labels.shape[1]  # number of classes\n",
    "model_perf = utils.model_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    name = 'softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['regularization'] = 5e-4\n",
    "    params['dropout']        = 0.5\n",
    "    params['learning_rate']  = 0.2\n",
    "    params['decay_rate']     = 0.95\n",
    "    params['momentum']       = 0.9\n",
    "    params['F']              = []\n",
    "    params['F_0']            = 20\n",
    "    params['K']              = []\n",
    "    params['p']              = []\n",
    "    params['M']              = [C] \n",
    "    params['train_laplacians'] = train_laplacians\n",
    "    params['test_laplacians'] = val_laplacians\n",
    "    params['val_laplacians'] = test_laplacians\n",
    "    model_perf.test(models.cgcnn(L, num_labels_per_image, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
